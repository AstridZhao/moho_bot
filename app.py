from langchain.vectorstores import Epsilla
from pyepsilla import vectordb
from sentence_transformers import SentenceTransformer
import streamlit as st
import os
import subprocess, threading
from typing import List
from transformers import AutoTokenizer
import requests
import json
import time, queue

import subprocess, os
from langchain.vectorstores import Epsilla

# start docker ->---------------->---------------->---------------
# if (os.getcwd() != "/Users/astridz"):
#     os.chdir('/Users/astridz')
# # Pull the Docker image

# start_command = "open -a Docker"
# subprocess.run(start_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
# # out, err = start_process.communicate()
# # if start_process.returncode != 0:
# #     print(f"Error start Docker: {err}")
# # else:
# #     print("start success! \n")
# #     print(f"Output: {out} \n")

# pull_command = "docker pull epsilla/vectordb"
# subprocess.run(pull_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
# # out, err = pull_process.communicate()
# # if pull_process.returncode != 0:
# #     print(f"Error pulling Docker image: {err}")
# # else:
# #     print("pull success! \n")
# #     print(f"Output: {out} \n")

# # # Run the Docker container
# run_command = "docker run --pull=always -d -p 8888:8888 epsilla/vectordb"
# subprocess.run(run_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
# # out, err = run_process.communicate()
# # if run_process.returncode != 0:
# #     print(f"Error running Docker container: {err}")
# # else:
# #     print("r! success! \n")
# #     print(f"Output: {out} \n")
    
# os.chdir('/Users/astridz/Documents/moho_bot')

# ->---------------->---------------->---------------

local_directory = "/Users/astridz/Documents/moho_bot"
# Local embedding model for embedding the question.
model = SentenceTransformer('all-MiniLM-L6-v2')
class LocalEmbeddings():
    def embed_query(self, text: str) -> List[float]:
        return model.encode(text).tolist()
    
embeddings = LocalEmbeddings()
# Connect to Epsilla as knowledge base.
client = vectordb.Client()
vector_store = Epsilla(
    client,
    embeddings,
    db_path="/tmp/localchatdb",
    db_name="LocalChatDB"
)

vector_store.use_collection("LocalChatCollection")

model_name = 'TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf'
pure_name = model_name.split('/')[-1]

# def generate_response(process):
    # output,error = process.communicate()
    #     # #reinitialize assistant_response each time
    #     # if process.poll() is not None and output == '':
    #     #     print("Subprocess has completed.")
    #     #     break 
    # if output:
    #     print("Subprocess has started.")
    #     assistant_response = f"{output.strip()}"
        
    #     # Ensure that `messages` is initialized in `st.session_state`
    #     if "messages" not in st.session_state:
    #         st.session_state["messages"] = []
            
    #     if assistant_response:
    #         msg = { 'role': 'assistant', 'content':assistant_response}
    #         st.session_state.messages.append(msg)
            
    #         # Ensure that Streamlit commands are only called in the main thread
    #         if threading.current_thread() == threading.main_thread():
    #             st.chat_message("assistant").write(msg['content'])
    #         print(f'{assistant_response}\n')
    # else:
    #     print(error)
        
    # process.stdout.close()

st.set_page_config(page_title="ðŸ¦™ðŸ’¬ Llama 2 Chatbot with Streamlit")

#Create a Side bar
with st.sidebar:
    st.title("ðŸ¦™ðŸ’¬ Llama 2 MohoBot")
    st.header("Settings")
    
# Clear the Chat Messages
def clear_chat_history():
    st.session_state.messages=[{"role":"assistant", "content": "How may I assist you today?"}]
    print_dialog()
# create Clear button
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

# Diplay the chat messages
def print_dialog(): 
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

SYSTEM_PROMPT = f'''You are a helpful question answer assistant. Answer the user question followed the rules:1. Do not copy the context in your answer. Try to understand the Context and rephrase them. 2. Please don't make things up or say things not mentioned in the Context. 3. You can trust the context. 4. Give a short response! Your answer should be in 200 words. The context is: "'''

#store message 
if "messages" not in st.session_state.keys():
    st.session_state.messages=[{"role": "assistant", "content": "You are a helpful question answer assistant. How can I help you?"}]
    # st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]
    st.chat_message("assistant").write("I am a helpful question answer assistant. How can I help you?")

if question := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": question})
    print_dialog()
    # https://github.com/epsilla-cloud/app-gallery/blob/main/local-chatbot/app.py
    #https://medium.com/@anoopjohny2000/building-a-conversational-chat-interface-with-streamlit-and-langchain-for-csvs-8c150b1f982d?source=read_next_recirc-----a5053b16f85----2---------------------81756fa8_2a08_42cb_bb9e_43cd23b2ae3a-------

    #vector_store.similarity_search
    context = ''.join(map(lambda doc: doc.page_content, vector_store.similarity_search(question, k = 2)))
    
    #Testing:
    print(context)
    
    prompt_template = f'''[INST]<<SYS>>\n{SYSTEM_PROMPT}{context}\n<</SYS>>\n\n{question}[/INST]'''
    # prompt_template = f'''{SYSTEM_PROMPT}{context} USER: {question} ASSISTANT: 

    # Start the subprocess and the threading to handle its output
    if (os.getcwd() != '/Users/astridz/Documents/llama.cpp'):
        os.chdir('/Users/astridz/Documents/llama.cpp')

    # ./main -m llama-2-7b-chat.Q4_K_M.gguf -c 1024 -b 16 -ngl 48 -p
    pure_name = "llama-2-7b-chat.Q4_K_M.gguf"
    command = ['./main', '-m', pure_name, '-c', '2048', '--keep', '0', '-b', '256', '-ngl', '48', '-p', prompt_template]
 
    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE,text=True)
    
    start_time = time.time()
    # Function to read from a stream and put the output in a queue
    def enqueue_output(stream, queue):
        for line in iter(stream.readline, b''):
            queue.put(line)
        stream.close()
    
    # Create queues to hold the output
    stdout_queue = queue.Queue()
    stdout_thread = threading.Thread(target=enqueue_output, args=(process.stdout, stdout_queue))
    stdout_thread.start()
    
    # Read from the queues
    while True:
        try:
            # Try to get output from the queues
            stdout_line = stdout_queue.get_nowait()
            print(stdout_line)
            st.session_state.messages.append([{"role":"assistant", "content": stdout_line}])
            st.chat_message("assistant").write(stdout_line)
                    # output = process.stdout.read()
            
        except queue.Empty:
        # No output ready
            pass

    # Process the output here

        # Check if the subprocess is still running
        if process.poll() is not None:
            break

    # Make sure threads have finished
    stdout_thread.join()

    process.stdout.close()
        
    end_time = time.time()
    elapsed_time = end_time - start_time
    print("total time :",  elapsed_time)
    
    # # output = process.stdout.read()
    # end_time = time.time()
    # elapsed_time = end_time - start_time
    # print("total time :",  elapsed_time)

    # marker_index = output.find("[/INST]")
    # if marker_index != -1:
    #     assistant_response = output[marker_index + len("[/INST]"):] 
    #     st.session_state.messages.append([{"role":"assistant", "content": assistant_response}])
    #     st.chat_message("assistant").write(assistant_response)
    
  
        # if process.poll() is not None and output == '':
        #     print("Subprocess has completed.")
        #     break 
        # if output:
        #     assistant_response = output.strip()
        #     st.session_state.messages.append([{"role":"assistant", "content": assistant_response}])
        #     st.chat_message("assistant").write(assistant_response)















    
    # # # Start the thread that will handle the subprocess output
    # output_thread = threading.Thread(target=generate_response, args=(process, output_log))
    # output_thread.start()
    # # Wait for the subprocess and thread to finish
    # process.wait()
    # output_thread.join()
    # os.chdir(local_directory)
    
    # placeholder=st.empty()
    # full_response=''
    
